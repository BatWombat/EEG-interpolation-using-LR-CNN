#Importing libraries
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from keras.models import Model
from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from tensorflow.keras.optimizers import Adam
import sklearn as sk
from __future__ import absolute_import, division, print_function


def build_model():
    # Creating the inputshape for the tensorflow model
    input_shape = (X_train.shape[1],X_train.shape[2])

    #Making a sequential model
    model = keras.Sequential([

        #convolutional layer with filter 32 and kernel 3
        layers.Conv1D(32, 3, activation='relu', input_shape=input_shape),
        #Normalizing the ouput of convolutional layer
        layers.BatchNormalization(),
        #Adding a Maxpooling layer with kernel 1
        layers.MaxPooling1D(1),
        
        #Same as code above, convolutional layer has filter 64, kernel 3. Max pooling layer is kernel 2
        layers.Conv1D(64, 3, activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling1D(2),

        # Flatten the ouput of the previous maxpooling layer
        layers.Flatten(),

        #Normal Dense layer with filter 64 
        layers.Dense(64, activation='relu'),
        #Normalizing output of dense layer
        layers.BatchNormalization(),
        
        #Dense layer with filter 32
        layers.Dense(32, activation='relu'),
        #Normalizing output of dense layer
        layers.BatchNormalization(),
        

        #Last layer, a dense layer with filter 1, as an output lauyer
        layers.Dense(1, activation='linear')
    ])

    #Compiling the modile, validation loss as MSE, optimizer is Adam with learning rate 0.001, this is default. Evaluation metrics is determined as MSE
    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['mse'])
    
    return model
    
    
#Making class so a dot represents an epoch when fitting the model.
#from: https://www.youtube.com/watch?v=-vHQub0NXI4
class PrintDot(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs):
        if epoch % 100 == 0:
            print("")
        print(".", end='')
        
        
#Normalizing function for the data
#from: https://www.youtube.com/watch?v=-vHQub0NXI4
def norm(X):
    X_stats = X.describe()
    X_stats = X_stats.transpose()
    return (X - X_stats['mean']) / X_stats['std']
    
    
#Assigning input feautures to list
cols = ["Fz", "Cz", "Pz", "C3", "F3", "C4", "P3", "F4", "P4", 'Timestamp']

#Assigning datasets to list
datalist_train = ["Baseline.csv", "Plausible.csv", "Implausible.csv"]
datalist_test = ["2019_Control.csv", "2019_Script-Related.csv", "2019_Script-Unrelated.csv"]

#Making a early_stop method, monitoring the MSE as validation loss, with a patience of 10 epochs
early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)

#creating a loop for the different datasets
for j, filename in enumerate(datalist_train):
    #loading the corresponding training data and testing data
    data_train = pd.read_csv('Datasets/{}'.format(filename))
    data_test = pd.read_csv('Datasets/{}'.format(datalist_test[j]))
    
    #assigning timestamp to a variable for later use
    timestamps = data_test['Timestamp']
    
    #Configuring the layout of the plots
    fig, axs = plt.subplots(3, 3, figsize = (10, 10))
    axs = axs.flatten()
    
    #creating a loop in the different input features in the dataset
    for i, col in enumerate(cols):
        #dropping the electrode to be predicted from the input feautres
        X_train = data_train[cols].drop(col, axis = 1)
        #Making sure that the timestamp will not be predicted
        if col == 'Timestamp':
            continue
        #adding the selected electrode as the target variable for the training dataset
        y_train = data_train[col]
        
        #Normalizing input training data
        X_train_normed = norm(X_train)
        X_train = np.reshape(X_train_normed, (X_train_normed.shape[0], X_train_normed.shape[1], 1))
        
        #Assigning the CNN model to a variable called model
        model = build_model()
        
        #Making a checkpoint, this keeps track of the epoch with the lowest MSE
        checkpoint = ModelCheckpoint(model, monitor='val_loss', save_best_only=True)
        
        #Assigning number of maximum epcohs
        EPOCHS = 100
        #Fitting the model, with validation split of 20%, calling the early_stop and PrintDot methods, and assigning to variable history
        history = model.fit(X_train, y_train, epochs=EPOCHS, verbose=0, validation_split=0.2, callbacks=[early_stop, PrintDot()])
        
        #dropping the electrode to be predicted from the input feautres and adding to the target variable for the testing data
        X_test = data_test[cols].drop(col, axis = 1)
        y_test = data_test[col]
        
        #Normalizing input test data
        X_test_normed = norm(X_test)
        X_test = np.reshape(X_test_normed, (X_test_normed.shape[0], X_test_normed.shape[1], 1))        
        
        #predicting the target electrode
        y_pred = model.predict(X_test)
        #Reshaping predicted data for grouping and averaging
        y_pred = y_pred.reshape(-1,)
        #grouping true voltages with their corresponding timestamp and then taking the mean of every timestamp
        actual_means = y_test.groupby(timestamps).mean()
        #changing the type of predicted voltages to pandas series, grouping with corresponding timestamp and then taking the mean of every timestamp
        pred_means = pd.Series(y_pred, index = y_test.index).groupby(timestamps).mean()
        #calculating the mse of the averages (this is the average mse over all trials) 
        mse = mean_squared_error(actual_means, pred_means)

        #plotting the average true and predicted EEG signal by Voltage against corresponding timestamp
        axs[i].plot(actual_means.index, actual_means.values, label="Means of Actual Voltages")
        axs[i].plot(pred_means.index, pred_means.values, label="Means of Predicted Voltagess", c="r")
        #adding legend
        axs[i].legend()
        #adding title of target electrode with MSE score
        axs[i].set_title("{} MSE score: {:.2f}".format(col, mse))
        #labeling x-axis
        axs[i].set_xlabel("Timestamp (ms)")
        #labeling y-axis
        axs[i].set_ylabel("Voltage (Î¼V)".format(col))
        #inverting y-axis, this is common for EEG and ERP signals
        axs[i].invert_yaxis()
    #adding title of the condition
    fig.suptitle("EEG interpolation of {} condition".format(datalist_test[j][5:-4]))
    #fitting plots in the figure area
    plt.tight_layout()
    
    
